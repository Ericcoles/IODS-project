dist_man <- dist(boston_scaled, method = 'manhattan')
# look at the summary of the distances
summary(dist_man)
summary(dist_eu)
# k-means clustering
km <-kmeans(boston_scaled, centers = 3)
# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)
# MASS, ggplot2 and Boston dataset are available
set.seed(123)
# determine the number of clusters
k_max <- 10
# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})
# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')
# k-means clustering
km <-kmeans(boston_scaled, centers = 2)
# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)
library(MASS)
library(GGally)
library(ggplot2)
library(tidyr)
library(corrplot)
data("Boston")
str(Boston)
dim(Boston)
summary(Boston)
cor_matrix<-cor(Boston)
corrplot(cor_matrix, method="circle")
cor_matrix<-cor(Boston) %>% round(digits = 2)
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)
cor_matrix
# center and standardize variables
boston_scaled <- scale(Boston)
# summaries of the scaled variables
summary(boston_scaled)
# class of the boston_scaled object
class(boston_scaled)
# change the object to data frame
boston_scaled<-as.data.frame(boston_scaled)
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins
# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))
# look at the table of the new factor crime
table(crime)
# summary of the scaled crime rate
summary(crime)
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)
# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
# boston_scaled is available
# number of rows in the Boston dataset
n <- nrow(boston_scaled)
# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)
# create train set
train <- boston_scaled[ind,]
# create test set
test <- boston_scaled[-ind,]
# save the correct classes from test data
correct_classes <- test$crime
# remove the crime variable from test data
test <- dplyr::select(test, -crime)
# linear discriminant analysis
lda.fit <- lda(crime~., data = train)
# print the lda.fit object
lda.fit
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(train$crime)
# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
# lda.fit, correct_classes and test are available
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
# lda.fit, correct_classes and test are available
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)
# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class) %>% addmargins
# Percentage inaccuracy
table(correct = correct_classes, predicted = lda.pred$class) %>% prop.table() %>% addmargins()
loss_func <- function(class, prob) {
n_wrong <- class != prob
mean(n_wrong)
}
loss_func(class = correct_classes, prob = lda.pred$class)
install.packages(plotly)
install.packages("plotly")
library(MASS)
data('Boston')
#scaled
boston_scaledx=scale(Boston)
km2 <-kmeans(boston_scaledx, centers = 3)
boston_scaledx <- as.data.frame(boston_scaledx)
#LDA
lda.fit2 <- lda(km2$cluster ~., data=boston_scaledx)
#lda biplot
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
heads <- coef(x)
arrows(x0 = 0, y0 = 0,
x1 = myscale * heads[,choices[1]],
y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
text(myscale * heads[,choices], labels = row.names(heads),
cex = tex, col=color, pos=3)
}
# target classes as numeric
classes <- as.numeric(km2$cluster)
# plot the lda results
plot(lda.fit2, dimen = 2, col=classes, pch= classes)
lda.arrows(lda.fit, myscale = 1)
#First run the code below for the (scaled) train data that you used to fit the LDA. The code creates a matrix product, which is a projection of the data points.
model_predictors <- dplyr::select(train, -crime)
# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)
library(plotly)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = train$crime)
#First run the code below for the (scaled) train data that you used to fit the LDA. The code creates a matrix product, which is a projection of the data points.
model_predictors <- dplyr::select(train, -crime)
# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)
library(plotly)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color = train$crime)
# Saving the analysis daatset, name it as human.txt, under the data folder
write.table(human, file = "data/human.txt")
setwd("~/")
setwd("~/")
# Saving the analysis daatset, name it as human.txt, under the data folder
# setwd("~/Documents/GitHub/IODS-project/data")
write.table(human, file = "data/human.txt")
# Saving the analysis daatset, name it as human.txt, under the data folder
# setwd("~/Documents/GitHub/IODS-project/data")
write.table(create_human, file = "data/human.txt")
# Saving the analysis daatset, name it as human.txt, under the data folder
# setwd("~/Documents/GitHub/IODS-project/data")
write.table(create_human.r, file = "data/human.txt")
# Saving the analysis daatset, name it as human.txt, under the data folder
# setwd("~/Documents/GitHub/IODS-project/data")
write.table(create_human.R, file = "data/human.txt")
# read the data and check for the validity
create_human<- read.csv(file="create_human.csv")
# read the data and check for the validity
create_human<- read.csv(file="create_human.R")
---
title: "Chapter5"
output: html_document
---
# read the human data
human <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human1.txt", sep  =",", header = T)
# look at the (column) names of human
names(human)
# look at the structure of human
str(human)
# print out summaries of the variables
summary(human)
# access the stringr package
library(stringr)
# look at the structure of the GNI column in 'human'
str(human$GNI)
# remove the commas from GNI and print out a numeric version of it
str_replace(human$GNI, pattern=",", replace ="") %>% as.numeric
# human with modified GNI and dplyr are available
# columns to keep
keep <- c("Country", "Edu2.FM", "Labo.FM", "Life.Exp", "Edu.Exp", "GNI", "Mat.Mor", "Ado.Birth", "Parli.F")
# select the 'keep' columns
human <- select(human, one_of(keep))
# print out a completeness indicator of the 'human' data
complete.cases(human)
# print out the data along with a completeness indicator as the last column
data.frame(human[-1], comp = complete.cases(human))
# filter out all rows with NA values
human_ <- filter(human, complete.cases(human))
# human without NA is available
# look at the last 10 observations
tail(human, 10)
# last indice we want to keep
last <- nrow(human) - 7
# choose everything until the last 7 observations
human_ <- human[1:last, ]
# add countries as rownames
rownames(human) <- human$Country
# modified human, dplyr and the corrplot functions are available
# remove the Country variable
human_ <- select(human, -Country)
# Access GGally
library(GGally)
# visualize the 'human_' variables
ggpairs(human_)
# compute the correlation matrix and visualize it with corrplot
cor(human_) %>% corrplot
# modified human is available
# standardize the variables
human_std <- scale(human)
# print out summaries of the standardized variables
summary(human_std)
# perform principal component analysis (with the SVD method)
pca_human <- prcomp(human_std)
# draw a biplot of the principal component representation and the original variables
biplot(pca_human, choices = 1:2, cex = c(0.8, 1), col = c("grey40", "deeppink2"))
# pca_human, dplyr are available
# create and print out a summary of pca_human
s <- summary(pca_human)
s
# rounded percentages of variance captured by each PC
pca_pr <- round(100*s$importance[2,], digits = 1)
# print out the percentages of variance
pca_pr
# create object pc_lab to be used as axis labels
pc_lab <- paste0(names(pca_pr), " (", pca_pr, "%)")
# draw a biplot
biplot(pca_human, cex = c(0.8, 1), col = c("grey40", "deeppink2"), xlab = pc_lab[1], ylab = pc_lab[2])
# the tea dataset and packages FactoMineR, ggplot2, dplyr and tidyr are available
# column names to keep in the dataset
keep_columns <- c("Tea", "How", "how", "sugar", "where", "lunch")
# select the 'keep_columns' to create a new dataset
tea_time <- select(tea, one_of(keep_columns))
# look at the summaries and structure of the data
summary(tea_time)
str(tea_time)
# visualize the dataset
gather(tea_time) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
human <- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human1.txt", sep  =",", header = T)
# look at the (column) names of human
names(human)
# look at the structure of human
str(human)
# print out summaries of the variables
summary(human)
library(stringr)
str(human$GNI)
# remove the commas from GNI and print out a numeric version of it
str_replace(human$GNI, pattern=",", replace ="") %>% as.numeric
# human with modified GNI and dplyr are available
# columns to keep
keep <- c("Country", "Edu2.FM", "Labo.FM", "Life.Exp", "Edu.Exp", "GNI", "Mat.Mor", "Ado.Birth", "Parli.F")
human <- select(human, one_of(keep))
# print out a completeness indicator of the 'human' data
complete.cases(human)
# print out the data along with a completeness indicator as the last column
data.frame(human[-1], comp = complete.cases(human))
# filter out all rows with NA values
human_ <- filter(human, complete.cases(human))
tail(human, 10)
# last indice we want to keep
last <- nrow(human) - 7
# choose everything until the last 7 observations
human_ <- human[1:last, ]
# add countries as rownames
rownames(human) <- human$Country
# modified human, dplyr and the corrplot functions are available
# remove the Country variable
human_ <- select(human, -Country)
tail(human, 10)
# last indice we want to keep
last <- nrow(human) - 7
# choose everything until the last 7 observations
human_ <- human[1:last, ]
# add countries as rownames
rownames(human) <- human$Country
# modified human, dplyr and the corrplot functions are available
library(dplyr)
# remove the Country variable
human_ <- dplyr::select(human, -Country)
library(GGally)
# visualize the 'human_' variables
ggpairs(human_)
library(GGally)
library(MASS)
# visualize the 'human_' variables
ggpairs(human_)
library(dplyr)
# remove the Country variable
human_ <- dplyr::select(human, -Country)
# Access GGally
library(MASS)
library(corrplot)
library(tidyverse)
library(tidyr)
library(GGally)
library(ggplot2)
# visualize the 'human_' variables
ggpairs(human_)
# compute the correlation matrix and visualize it with corrplot
cor(human_) %>% corrplot
install.package(tidyverse)
#Created on 30.11.2017
#Eric Coles
#RStudio exercise 5 -- Data wrangling
#Data source, United Nations Development Programme.
hd <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/human_development.csv", stringsAsFactors = F)
gii <- read.csv("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/gender_inequality.csv", stringsAsFactors = F, na.strings = "..")
str(hd)
dim(hd)
summary(hd)
str(gii)
dim(gii)
summary(gii)
# Modify the column names of data
colnames(hd) <- c("hdir", "cnty", "hdi", "life", "edu", "edu.mean", "gni", "gni")
colnames(gii) <- c("giirnk", "cty", "gii", "mort.M", "t.birth", "Par.M", "edu2F", "edu2M", "labF", "labM")
# Modify the column names of Human Development dataset and Gender inequality data
column_names_hd=c('HDI.Rank','Country','HDI','Life.Exp','Edu.Exp','Edu.Mean','GNI','GNI.Minus.Rank')
colnames(hd)=column_names_hd
column_names_gii=c('GII.Rank','Country','GII','Mat.Mor','Ado.Birth','Parli.F','Edu2.F','Edu2.M','Labo.F','Labo.M')
colnames(gii)=column_names_gii
# Calculate the ratio of female and male populations with secondary education and labour force participation in each country
gii=mutate(gii,Edu2.FM=Edu2.F/Edu2.M,Labo.FM=Labo.F/Labo.M)
# Join the two datasets
human=inner_join(hd,gii,by='Country')
dim(human)
summary(human)
# Mutate the GNI data in 'human' dataset
str(human$GNI)
human$GNI=str_replace(human$GNI,pattern=',',replace='') %>% as.numeric
# Dealing with not available (NA) values
keep=c("Country", "Edu2.FM", "Labo.FM", "Life.Exp", "Edu.Exp", "GNI", "Mat.Mor", "Ado.Birth", "Parli.F")
human=select(human,one_of(keep))
human=filter(human,complete.cases(human))
# Remove observations with related to regions instead of countries
last=nrow(human)-7
human_=human[1:last,]
# Define the row names of the data by the country names
rownames(human_)=human_$Country
human=human_[-1]
# Save data
setwd('/home/Ecoles/IODS_course/IODS-project/data')
write.table(human,file='human.txt',row.names=T,sep = '\t')
# Save data
setwd('/Users/eric/Documents/GitHub/IODS-project/data')
write.table(human,file='human.txt',row.names=T,sep = '\t')
human = read.table('/Users/eric/Documents/GitHub/IODS-project/data',sep='\t',header = TRUE)
str(human)
dim(human)
# Access GGally
library(MASS)
library(corrplot)
install.package(tidyverse)
library(tidyverse)
library(tidyr)
library(GGally)
library(ggplot2)
ggpairs(human)
cor_matrix=cor(human)
corrplot(cor_matrix,type='upper')
url<-"http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/alc.txt"
alc<- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/alc.txt", sep="\t", header=TRUE)
I read the joined student alcohol consumption data into R from  the url. [Reference](http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/alc.txt)
```{r}
url<-"http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/alc.txt"
alc<- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/alc.txt", sep="\t", header=TRUE)
```
Data Set Information:
The data set focuses on student achievement in secondary education of two portuguese schools. The data set includes student grades, demographic information, and social and school related features. The data was collected using school reports and questionnaires. There are two datasets, math and portuguese language with grades from different grading period data; G1, G2 and G3.
Printed out the names of the variables in the data, explored their sturcture and dim.
```{r alc, echo=TRUE}
colnames(alc)
str(alc)
dim(alc)
```
I have chosen studytime, traveltime, age and freetime. I predict that traveltime  increases alcohol consumpiton. I predict that studytime has a negative correlation with high_use alcohol consumption. I predict that age is related to alchohol consumption as well as freetime.
The variable "studytime" is numeric, the variable "traveltime" is numeric, the variable "age" is an integer, and the variable "freetime" is numeric.
summary(alc$studytime)
summary(alc$traveltime)
summary(alc$age)
summary(alc$freetime)
Minimum value of studytime is 1.0, mean 2.037 and max 4.0.
Minimum value of traveltime is 1.0, mean 1.448 and max 4.0.
Minimum value of age is 15, mean 16.59 and max 22.0.
Minimum value of freetime is 1.0, mean 3.22 and max 4.0.
I graphically explored the varabiles;"high_use","studytime","traveltime","age","freetime"
```{r}
alc_data <- select(alc,c("high_use","studytime","traveltime","age","freetime"))
gather(alc_data) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()
```
I created several scattered plots using the variables;"high_use","studytime","traveltime","age","freetime".
```{r}
p<-ggpairs(alc_data)
p
```
From the scatter plots, one can see that there is a postive correlation between the target variable "high_use" and the independent variables "traveltime","age", and "freetime" and that there is a negative correlation between "studytime" and "high_use".
I defined a regression model with the variables "high_use","studytime","traveltime","age","freetime".
```{r}
m <- glm(high_use ~ high_use + studytime + traveltime + age + freetime, data = alc_data, family = "binomial")
summary(m)
coef(m)
```
All of the variables were significant. "studytime" had a p-value of 0.000639, "traveltime" had a p-value of 0.35353, "age" had a p-value of 0.044513, and "freetime" had a p-value 0.011030
I obtain the odds ratios and their confidence intervals by taking the exponential function:
```{r}
OR <- coef(m) %>% exp
CI <- confint(m) %>% exp
cbind(OR, CI)
```
The odds of high_use alcohol is less than 50% with study_time in study population effect between 42% and 79%, supporting my intitial hypothesis.
The odds ratio for traveltime, age, and freetime are greater than one suggesting that there is an relationship between high_use alcohol and these variables however the confidence interval 95%(97.5%-2.5%) crosses 1, implying there is no relationship between these variables and high_use alcohol. These results suggest I should reject my intitial hypothesis for traveltime, age, and freetime.
predict() the probability of high_use
```{r}
probabilities <- predict(m, type = "response")
```
add the predicted probabilities to 'alc'
```{r}
alc_data <- mutate(alc_data, probability = probabilities)
```
use the probabilities to make a prediction of high_use
```{r}
alc_data <- mutate(alc_data, prediction = probability > 0.5)
```
see the last ten original classes, predicted probabilities, and class predictions
```{r}
select(alc_data, high_use, studytime, traveltime, age, freetime) %>% tail(10)
```
```{r}
table(high_use = alc_data$high_use, prediction = alc_data$prediction)
```
high_use FALSE TRUE
FALSE   256   12
TRUE     96   18
#10-fold cross-validation
```{r}
loss_func <- function(class, prob) {
n_wrong <- abs(class - prob) > 0.5
mean(n_wrong)
}
loss_func(class = alc_data$high_use, prob = alc_data$probability)
library.tidyr
library(boot)
cv <- cv.glm(data=alc_data, cost=loss_func, glmfit=m, K=10)
cv$delta[1]
```
The datacamp model had [1] 0.2539267 as cv value while my model has a cv value of 0.2958115. The results showed that around 70% of the predictions are  correct, and 30% of the predictions are incorrect. The model is better than simple guessing strategy, where the odds of guessing correctly would be approximately 50% however my model has a larger prediction error than data camp's model.
#Super-Bonus
```{r}
m2 <- glm(high_use ~ high_use + studytime + traveltime + age + freetime + health +famrel + G1 +G2 + G3, data = alc, family = "binomial")
probabilities2 <- predict(m2, type = "response")
alc_data2 <- select(alc,c("high_use","studytime","traveltime","age","freetime","health","famrel","G1","G2","G3"))
alc_data2<- mutate(alc_data2, probability2 = probabilities2)
alc_data2 <- mutate(alc_data2, prediction2 = probability2 > 0.5)
loss_func <- function(class, prob) {
n_wrong <- abs(class - prob) > 0.5
mean(n_wrong)
}
loss_func(class = alc_data2$high_use, prob = alc_data2$probability2)
cv2 <- cv.glm(data = alc_data2, cost = loss_func, glmfit = m2, K = 10)
cv2$delta[1]
```
After adding more variables to the model, the prediction error rose to over 31%. This leads me to believe that not enough variables were used to have a significant change in the model.
I loaded "GGally","ggplot2"and "tidyr" libaries for this assignment.
```{r}
library(GGally)
library(ggplot2)
library(tidyr)
library(dplyr);
```
I read the joined student alcohol consumption data into R from  the url. [Reference](http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/alc.txt)
```{r}
url<-"http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/alc.txt"
alc<- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/alc.txt", sep="\t", header=TRUE)
```
Data Set Information:
The data set focuses on student achievement in secondary education of two portuguese schools. The data set includes student grades, demographic information, and social and school related features. The data was collected using school reports and questionnaires. There are two datasets, math and portuguese language with grades from different grading period data; G1, G2 and G3.
Printed out the names of the variables in the data, explored their sturcture and dim.
```{r alc, echo=TRUE}
colnames(alc)
str(alc)
dim(alc)
```
I have chosen studytime, traveltime, age and freetime. I predict that traveltime  increases alcohol consumpiton. I predict that studytime has a negative correlation with high_use alcohol consumption. I predict that age is related to alchohol consumption as well as freetime.
The variable "studytime" is numeric, the variable "traveltime" is numeric, the variable "age" is an integer, and the variable "freetime" is numeric.
summary(alc$studytime)
summary(alc$traveltime)
summary(alc$age)
summary(alc$freetime)
Minimum value of studytime is 1.0, mean 2.037 and max 4.0.
Minimum value of traveltime is 1.0, mean 1.448 and max 4.0.
Minimum value of age is 15, mean 16.59 and max 22.0.
Minimum value of freetime is 1.0, mean 3.22 and max 4.0.
I graphically explored the varabiles;"high_use","studytime","traveltime","age","freetime"
```{r}
alc_data <- dplyr::select(alc,c("high_use","studytime","traveltime","age","freetime"))
gather(alc_data) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar()
alc<- read.table("http://s3.amazonaws.com/assets.datacamp.com/production/course_2218/datasets/alc.txt", sep="\t", header=TRUE)
summary(alc$studytime)
summary(alc$traveltime)
colnames(alc)
str(alc)
dim(alc)
